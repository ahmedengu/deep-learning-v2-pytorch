{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation\n",
    "\n",
    "Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren't able to generalize to data that hasn't been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I'll show you how to do this in PyTorch. \n",
    "\n",
    "As usual, let's start by loading the dataset through torchvision. You'll learn more about torchvision and loading data in a later part. This time we'll be taking advantage of the test set which you can get by setting `train=False` here:\n",
    "\n",
    "```python\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "```\n",
    "\n",
    "The test set contains images just like the training set. Typically you'll see 10-20% of the original dataset held out for testing and validation with the rest being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll create a model like normal, using the same one from my solution for part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)) and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "images, labels = next(iter(testloader))\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images))\n",
    "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities, we can get the most likely class using the `ps.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `ps.topk(1)`. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the predicted classes match the labels. This is simple to do by equating `top_class` and `labels`, but we have to be careful of the shapes. Here `top_class` is a 2D tensor with shape `(64, 1)` while `labels` is 1D with shape `(64)`. To get the equality to work out the way we want, `top_class` and `labels` must have the same shape.\n",
    "\n",
    "If we do\n",
    "\n",
    "```python\n",
    "equals = top_class == labels\n",
    "```\n",
    "\n",
    "`equals` will have shape `(64, 64)`, try it yourself. What it's doing is comparing the one element in each row of `top_class` with each element in `labels` which returns 64 True/False boolean values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the percentage of correct predictions. `equals` has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to `torch.mean`. If only it was that simple. If you try `torch.mean(equals)`, you'll get an error\n",
    "\n",
    "```\n",
    "RuntimeError: mean is not implemented for type torch.ByteTensor\n",
    "```\n",
    "\n",
    "This happens because `equals` has type `torch.ByteTensor` but `torch.mean` isn't implemented for tensors with that type. So we'll need to convert `equals` to a float tensor. Note that when we take `torch.mean` it returns a scalar tensor, to get the actual value as a float we'll need to do `accuracy.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 15.625\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print('Accuracy:', accuracy.item()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    ">**Exercise:** Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5112522320547846 Test acc: 81.38166666666666\n",
      "Val Loss: 0.44441148762110694 Val acc: 83.87\n",
      "Train Loss: 0.3861985291515205 Test acc: 86.00333333333333\n",
      "Val Loss: 0.4329503961619298 Val acc: 84.66\n",
      "Train Loss: 0.35194704932635273 Test acc: 87.13666666666666\n",
      "Val Loss: 0.41693054054193435 Val acc: 85.25\n",
      "Train Loss: 0.33144196805987025 Test acc: 87.81833333333333\n",
      "Val Loss: 0.3806680149523316 Val acc: 86.37\n",
      "Train Loss: 0.31664999543444944 Test acc: 88.21\n",
      "Val Loss: 0.3583174045108686 Val acc: 87.52\n",
      "Train Loss: 0.3016566251323167 Test acc: 88.83833333333332\n",
      "Val Loss: 0.3750533875860986 Val acc: 87.24\n",
      "Train Loss: 0.2907264505399824 Test acc: 89.32333333333334\n",
      "Val Loss: 0.36581371563255405 Val acc: 86.96000000000001\n",
      "Train Loss: 0.2805148991726355 Test acc: 89.645\n",
      "Val Loss: 0.3607267747828915 Val acc: 87.35000000000001\n",
      "Train Loss: 0.27613306655955594 Test acc: 89.84\n",
      "Val Loss: 0.3939716071839545 Val acc: 86.11999999999999\n",
      "Train Loss: 0.26689484872734115 Test acc: 90.115\n",
      "Val Loss: 0.36100544804220747 Val acc: 87.39\n",
      "Train Loss: 0.25798904852890003 Test acc: 90.31666666666666\n",
      "Val Loss: 0.36873549688014257 Val acc: 87.8\n",
      "Train Loss: 0.2511177398065832 Test acc: 90.66333333333333\n",
      "Val Loss: 0.39263643134551446 Val acc: 87.51\n",
      "Train Loss: 0.2431215016540688 Test acc: 91.03333333333333\n",
      "Val Loss: 0.36911397384610145 Val acc: 87.82\n",
      "Train Loss: 0.24283860311277514 Test acc: 91.07666666666667\n",
      "Val Loss: 0.3602194530758888 Val acc: 87.48\n",
      "Train Loss: 0.2368270052727987 Test acc: 91.12333333333333\n",
      "Val Loss: 0.39526702620231424 Val acc: 87.92\n",
      "Train Loss: 0.23256460883454091 Test acc: 91.30333333333334\n",
      "Val Loss: 0.35779398480418384 Val acc: 88.13\n",
      "Train Loss: 0.22646651703761075 Test acc: 91.56666666666666\n",
      "Val Loss: 0.3654390326730765 Val acc: 88.19\n",
      "Train Loss: 0.2214574310571146 Test acc: 91.68666666666667\n",
      "Val Loss: 0.3897086247611957 Val acc: 87.31\n",
      "Train Loss: 0.2148960081300438 Test acc: 91.915\n",
      "Val Loss: 0.38268496523237533 Val acc: 88.07000000000001\n",
      "Train Loss: 0.2113116664418788 Test acc: 92.06833333333333\n",
      "Val Loss: 0.38115448784676326 Val acc: 88.51\n",
      "Train Loss: 0.21014608785525948 Test acc: 92.15833333333333\n",
      "Val Loss: 0.43572505777049214 Val acc: 86.74\n",
      "Train Loss: 0.2059387914967467 Test acc: 92.46833333333333\n",
      "Val Loss: 0.37511837767187955 Val acc: 88.28\n",
      "Train Loss: 0.19842769435124358 Test acc: 92.60166666666667\n",
      "Val Loss: 0.4254857876877876 Val acc: 86.99\n",
      "Train Loss: 0.20471785131142908 Test acc: 92.46666666666667\n",
      "Val Loss: 0.39783773419393853 Val acc: 88.27000000000001\n",
      "Train Loss: 0.1916936133017164 Test acc: 92.86666666666666\n",
      "Val Loss: 0.4091127048821966 Val acc: 88.41\n",
      "Train Loss: 0.18979736429247965 Test acc: 92.87166666666667\n",
      "Val Loss: 0.40215304992191353 Val acc: 88.05\n",
      "Train Loss: 0.19178107777423758 Test acc: 92.78999999999999\n",
      "Val Loss: 0.40860767809638554 Val acc: 88.47\n",
      "Train Loss: 0.18555955403347388 Test acc: 93.17833333333333\n",
      "Val Loss: 0.4024825128874961 Val acc: 88.01\n",
      "Train Loss: 0.1848078378037349 Test acc: 93.13333333333334\n",
      "Val Loss: 0.42425851334052483 Val acc: 88.52\n",
      "Train Loss: 0.18116804541968334 Test acc: 93.28666666666666\n",
      "Val Loss: 0.3948348137508532 Val acc: 88.53\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += (torch.exp(log_ps).argmax(dim=1) == labels).sum().item()\n",
    "        \n",
    "    else:\n",
    "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
    "        \n",
    "        print('Train Loss:', running_loss/len(trainloader), 'Test acc:', running_acc/len(trainset)*100)\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for images,labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                running_loss += loss.item()\n",
    "                running_acc += (torch.exp(log_ps).argmax(dim=1) == labels).sum().item()\n",
    "            else:\n",
    "                print('Val Loss:', running_loss/len(testloader), 'Val acc:', running_acc/len(testset)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.\n",
    "\n",
    "<img src='assets/overfitting.png' width=450px>\n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add dropout to your model and train it on Fashion-MNIST again. See if you can get a lower validation loss or higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define your model with dropout added\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.5994423031806946 train acc 78.36999999999999\n",
      "test loss 0.45348099576439826 test acc 82.95\n",
      "train loss 0.43319723763064283 train acc 84.70833333333333\n",
      "test loss 0.4011837966313028 test acc 85.92\n",
      "train loss 0.3992949725944859 train acc 85.695\n",
      "test loss 0.39208154959283814 test acc 86.1\n",
      "train loss 0.37608113235184376 train acc 86.42166666666667\n",
      "test loss 0.36863697353442004 test acc 86.22\n",
      "train loss 0.35480475331197925 train acc 87.25500000000001\n",
      "test loss 0.38008442900742695 test acc 86.26\n",
      "train loss 0.3439371175984584 train acc 87.53666666666666\n",
      "test loss 0.3564842894768259 test acc 87.3\n",
      "train loss 0.3317562030243086 train acc 88.02\n",
      "test loss 0.36238113150095486 test acc 87.19\n",
      "train loss 0.3190693850360954 train acc 88.41499999999999\n",
      "test loss 0.3487393299865115 test acc 87.52\n",
      "train loss 0.31369214581210475 train acc 88.72666666666666\n",
      "test loss 0.3456991228043653 test acc 87.9\n",
      "train loss 0.3072972248262688 train acc 88.86333333333334\n",
      "test loss 0.34096994815738335 test acc 87.87\n",
      "train loss 0.2977258765589454 train acc 89.21333333333334\n",
      "test loss 0.3355929659810036 test acc 87.9\n",
      "train loss 0.2909246126710098 train acc 89.24333333333333\n",
      "test loss 0.34200572322128686 test acc 87.91\n",
      "train loss 0.284953982956501 train acc 89.52\n",
      "test loss 0.3366160397032264 test acc 88.17\n",
      "train loss 0.27847332176941036 train acc 89.67500000000001\n",
      "test loss 0.34525273949097673 test acc 88.14999999999999\n",
      "train loss 0.2768354187054293 train acc 89.82833333333333\n",
      "test loss 0.3396685538682968 test acc 88.27000000000001\n",
      "train loss 0.2693661250301134 train acc 90.03\n",
      "test loss 0.33566551897556157 test acc 88.2\n",
      "train loss 0.2662288905746901 train acc 90.135\n",
      "test loss 0.32360892951678316 test acc 88.57000000000001\n",
      "train loss 0.2632203990263916 train acc 90.24833333333333\n",
      "test loss 0.3367050989133537 test acc 88.64\n",
      "train loss 0.25578368904748194 train acc 90.515\n",
      "test loss 0.3278375987888901 test acc 88.62\n",
      "train loss 0.25466454899244345 train acc 90.64\n",
      "test loss 0.3254146303530711 test acc 88.82\n",
      "train loss 0.2512373190357296 train acc 90.71000000000001\n",
      "test loss 0.33452178181926157 test acc 88.86\n",
      "train loss 0.24773543746645516 train acc 90.78833333333334\n",
      "test loss 0.3392243545241417 test acc 88.46000000000001\n",
      "train loss 0.24320246604110388 train acc 90.90666666666667\n",
      "test loss 0.32297977126517874 test acc 89.13\n",
      "train loss 0.23998977608089125 train acc 91.20666666666666\n",
      "test loss 0.33245624725226386 test acc 88.44\n",
      "train loss 0.23779713421233936 train acc 91.11166666666666\n",
      "test loss 0.328154499601027 test acc 88.94999999999999\n",
      "train loss 0.2389819267064905 train acc 91.14333333333333\n",
      "test loss 0.3478125960678811 test acc 88.56\n",
      "train loss 0.2324059943495783 train acc 91.455\n",
      "test loss 0.3148240326506317 test acc 89.01\n",
      "train loss 0.23025898185016505 train acc 91.45333333333333\n",
      "test loss 0.33062067276732937 test acc 88.83\n",
      "train loss 0.22623385525127845 train acc 91.545\n",
      "test loss 0.3317220342007412 test acc 89.02\n",
      "train loss 0.22419995878503393 train acc 91.64\n",
      "test loss 0.3275951724143545 test acc 89.17\n"
     ]
    }
   ],
   "source": [
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    model.train()\n",
    "    for images,labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        ps = model(images)\n",
    "        loss = criterion(ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_acc += (torch.exp(ps).argmax(dim=1)==labels).sum().item()\n",
    "    else:\n",
    "        print('train loss', running_loss/len(trainloader), 'train acc',running_acc/len(trainset)*100)\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in testloader:\n",
    "                ps = model(images)\n",
    "                loss = criterion(ps,labels)\n",
    "                running_loss+= loss.item()\n",
    "                running_acc+= (torch.exp(ps).argmax(dim=1)==labels).sum().item()\n",
    "            else:\n",
    "                print('test loss',running_loss/len(testloader),'test acc', running_acc/len(testset)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADPCAYAAACgNEWWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XmYHWWZ9/Hvr5fsKyRsYQkBRIIgS0TwBcmIjIhoGGUUZJG5RuMyzjijjuIyyqAv4goqKuZlcGGTRVQWNyAmgJFAAgqyBAKEJWyB7Funl/v9o6rHQ9dTSXfSfbo6+X2uq68+5z5P1XnO6U6erqr73LciAjMzs6pp6O8JmJmZpXiBMjOzSvICZWZmleQFyszMKskLlJmZVZIXKDMzqyQvUGZWGZLOlnRZf89jc0j6saQvb+a2G33dkh6QNLXrWEm7S1otqXGzJl1xXqDMrK4kvVfSvPw/1uck/UbSkf00l5C0Jp/LYknfquJ/9hGxf0TMSsSfiogREdEOIGmWpPfXfYJ9xAuUmdWNpI8DFwDnAjsCuwPfB6b147ReGxEjgGOA9wIf6DpAUlPdZ2VeoMysPiSNBs4B/iUirouINRHRGhE3RMR/lmxzjaTnJa2QdJuk/WseO17Sg5JW5Uc/n8zj4yTdKGm5pKWSbpe0yf/rIuJh4HbgNfl+Fkn6tKT7gDWSmiTtlx+lLM9Pu72jy27GSbo5n9NsSXvUzPfbkp6WtFLSfElHddl2iKSr8m3vkfTamm0XSXpz4v2ZmB8FNkn6v8BRwIX5EeGFkr4n6ZtdtrlB0r9v6v2oAi9QZlYvRwBDgF/0YJvfAPsAOwD3AJfXPPY/wAcjYiTZojIzj38CeAYYT3aU9llgkzXdJE0m+w/+3prwKcDbgDGAgBuA3+fz+Vfgckn71ow/FfgSMA74c5f53g0cBGwHXAFcI2lIzePTgGtqHv+lpOZNzbtTRHyObIH9aH7a76PAT4BTOhdoSePIjhSv7O5++5MXKDOrl+2BlyKirbsbRMQlEbEqIlqAs4HX5kdiAK3AZEmjImJZRNxTE98Z2CM/Qrs9Nl509B5Jy8gWn4uBH9U89p2IeDoi1gGHAyOA8yJiQ0TMBG4kW8Q63RQRt+Xz/RxwhKTd8tdyWUS8HBFtEfFNYDBQu7jNj4hrI6IV+BbZYn54d9+rlIi4C1hBtigBnAzMiogXtmS/9eIFyszq5WWyU2Ddup4jqVHSeZIek7QSWJQ/NC7//i7geODJ/HTaEXn868BC4PeSHpd01iae6pCIGBsRe0XE5yOio+axp2tu7wI83eXxJ4EJqfERsRpYmm+HpE9Ieig/XbkcGF3zWrpu20F2FLjLJubeHT8BTstvnwZc2gv7rAsvUGZWL38C1gMndnP8e8lOe72Z7D/ziXlcABFxd0RMIzvd9kvg6jy+KiI+ERGTgLcDH5d0DJun9sjrWWC3LtezdgcW19zfrfOGpBFkp+ueza83fRp4NzA2IsaQHdmoZNsGYNf8OTd3vp0uA6bl17T2I3uvBgQvUGZWFxGxAvgC8D1JJ0oaJqlZ0lslfS2xyUighezIaxhZ5h8AkgZJOlXS6PyU2EqgM9X6BEl7S1JNvL0XXsJcYA3wqXzeU8kWwJ/VjDle0pGSBpFdi5obEU/nr6UNWAI0SfoCMKrL/g+V9M78CPPf89d+Zw/n+AIwqTYQEc+QXf+6FPh5frpyQPACZWZ1ExHfAj4OfJ7sP+ungY+S/qv+p2Sn0BYDD1L8z/p0YFF++u9D/O001j7ALcBqsqO276c+Q7QZc98AvAN4K/ASWXr8GXn2X6crgC+Sndo7lCxpAuB3ZAkfj+SvaT2vPH0I8CvgPcCy/LW9M198e+LbwEmSlkn6Tk38J8ABDKDTewByw0Izs62bpDeSneqb2OUaWqX5CMrMbCuWp6p/DLh4IC1O4AXKzGyrJWk/YDlZ2v0F/TydHvMpPjMzq6S61pc6tuEfvRpuqxpK6m929DC5SirGevhH1s0d1yR2YmZV4wKIZtuQcePGxcSJE/t7GraNmz9//ksRMX5T47xAmW1DJk6cyLx58/p7GraNk/Rkd8Y5ScLMzCrJC5SZmVWSFygzM6ukreYalAYPTsajpaXb+2ic/Kpk/OEPjU3Gx0xcnowPbk53E3j+6e0KsdH3p9u9bPdQet7Nq9OVT9SezmRb/qrhhdiYf+5aYSWz87CVyfhdvzogGd/9O38pxDrWrEmOLc3WS2XlQXlmnj8WYbbN8BGUmZlVkhcoMzOrJC9QZmZWSV6gzMyskrxAmZlZJQ24LD41pafck2w9gNW/nVSInbjr/OTY/VpGJ+MtHekMvKGNG5LxHXb7cyF21JsfSY6ds3afZPzqpw5JxncavioZb1xXzOLbf/RzybEr24Ym48eedFd63ycVK/ffuPDA5NiJ77kvGS/Nyuut2n29RNIc4IaI+MpGxkwEvhERJ9XEpgInRMQnu/Ecj5I15xsMzImIT/RwjtMjYkZPtjGrMh9BmW2CpN3IuqAe08dPtSIipkbEEcBBkib0cPvpfTEps/7iBcps004i60b6uKS9ACSdLelySb+RdJukYZ2DJTVI+qGkU2t3Iuk4SbdLmiPplLInk9QINAPrJTVJukLSbEm/lrRdPuZ8SXdImiVpT0kfBvbN7x/dB++BWd15gTLbtGOA3wNXki1WnRZExFuB24E357FG4GLg5oi4vHOgpAbgC/m+jgQ+lC9EtUZLmgX8FXgyIl4G/gF4KiKOBq4C/lXS64CdI+JI4IvAFyLiB/l8pkbE7NqdSpouaZ6keUuWLNniN8OsXrxAmW2EpF2BA4EbgM8AJ9Q8fG/+/Wmgs9zI64HtI+LaLrsaB+xDttDNzO93bTfQeYpvP2ClpKOAvYC788fnAnuXxEpFxIyImBIRU8aP32SHA7PKqEaSRA/K3URbuoxQmUcuOiwZ//ZelxViFz/7xuTYQQ3p59zQkX77Vm1Il11qbS9e+L8uDkqObVA6eWBQYzpJYE3boGR85KBi8shfl++SHFumsaGYDAEwqKE4l4++ZnZiJFzy0bcl4ztcOKdHc0nqabmknjkJ+FhE/CJ7Kl0sac/OZ6idRf59DnCHpK9ExGdqHn8JeAg4NiJaJTVHRLpuVWY5sB2wEHgd8HOyxe/RPHZiPq4z1nU+ZgNeNRYos+p6FzCt5v5MXnmaryAiLpD0X5I+S7ZgEREdkv4vcIukDmAJ8O4um3ae4iN//L+BDuCdkm4D1gCnRsRSSc9JugNoA/4p32aBpJ8DX4+IOzfz9ZpVhhcos42IiKO63L8iMeaimrsn5bEv1cRm5bHfAb/byHOlP1sA702M/Y9E7LSyfZsNRL4GZWZmleQFyszMKskLlJmZVVI1rkGVZFupuZiZFq3pMkJNE9KZaecfU7hkAMD8tXsWYkMa00lVZRl1QxrXJ+MjmtJll1L72dCRLumzujWdCVimbI6peEeks956+vpTWXwL1u6UHHvYGfcm44suTIbLSxqlSiD1U/mjgej+xSuYeNZNLDovnVVpViU+gjIzs0ryAmVmZpXkBcrMzCrJC5RZBUmaKGlJXvx1nqST+3tOZvXmBcqsumZHxFTgjcCn+nkuZnVX3yy+HjahK8vYS3nwv3bt0VQGq1hf76ixC5Nj564oZvxBzzPtUsqy+PpSWRZfW8lcGpSuxbfTkJWJfaf/5jlwxDPJ+J3/8fb0vs9P1+hTQ3HukZ7e1mQYsFbSsWQFa0cA10XEeZLGAFeTlURaDDwdEWfXbixpOnmvqMZRLhZrA4ePoMyq6+i8Nt99wI+AP0bEm8gKxJ4oaSjwAeDaiDgOSLZKrq1m3jgs3R3arIq8QJlVV+cpvonAmcDBkm4hq+03CdiBrPXG/Hz83YU9mA1gXqDMKi4iNpC18/gy8G/A3wFP5bHHgIPzoYf2ywTN+kg1KkmYWUrnKb7BwI1k15iuAu4na70BWffeayT9I/Ai8HA/zNOsT3iBMqugiFhEseMuwI9r7+St5N8SEe2SvkzWzNBsq1DfBaoP060OnvxEMr6mI51pN7ppbSG2tiPdlXZwWUfdxnTW24qWocl4ct9N6X2XZdr1VGo/ZbX1Okg/5y5DVyXjIxqLNQfL3sPnNoxJxvc88bFkfN35yXCPOypvA4YCv5Uk4AXgnI0NPmDCaOa5Dp8NED6CMhvAImINcNQmB5oNQE6SMDOzSvICZWZmleQFyszMKmnAXYNqHJ8u1fKOHf6SjD/bmr44P3nI4kJs0Yb0voeWNPIrSypY25ZOFOiJlvb0j6axpOxQU0M63pMkibaO9N8rg0qSRA4b8Xghdu/aPZJjG0g/545Di+WSABYloyVUklBS0gjTzAYGH0GZ9RJJoyTdkFcgv0tSutBg9/c3VdI3emt+ZgPNgDuCMquw04HfRsT38rTvuhe+k9QQsQ2Uz7Vtgo+gzHrPWuAwSTtGZrmkhyRdLuleSacDSJok6Xf5kdb5eewASTMlzZF0Ye1OJQ2RdK2kN+W3L8vHXp8ftU2UdLuka4BP1v9lm/UNL1BmvedSYAHwu3yh2QfYCfgw2WeVPpKP+yrwkbwQbJOkKWQVII6JiDcAu+TbQtZq4wrggoiYCbwfmJlXNf8JeRsNYBfg1Ij4WtdJSZqeNz2ct2TJkt5/1WZ9xKf4zHpJRLQB5wLnSvo7sqoOj0fESoD8tB/AvsD/5HdHAreS1db7lqRhwJ5kCw7ANLLeT3fk9ycDr5N0BtAM3J7H/5IXlU3NawYwA2DKlCnOHLEBo86ljtL/NtScznpLNSx88v37JEbCmMY/JeMbIv0SxzQUSx2NbFiXHFvWsK8su238kNXJ+Ojm4v5LmwdGuozS8ER5IYCWjvTrXNdefG/XtTcnx64vyRx8cf3IZHzDyOIcxzWlX/uj63ZIxg8d+WQyvujwNyfj3HlfMVaRbD1JewDP5QvFi2RnKFKTWwB8MiKezBetRuB84LsR8WtJ18H/poheCTRK+lBEXERWDPZPEXFp/pzNwASyhoVmWxWf4jPrPQcAt+UVyL8HfKlk3KeBiyTNBG4mO1q6Afi6pJ+TLVi1Pg4cIuk0siOhY/NrUDOBv+/9l2FWDT7FZ9ZLIuJGsrYYtabUPH54/v1x4K1dxj0F7J/Y7az8+/Sa2BmJcSf1ZK5mA4GPoMzMrJK8QJmZWSV5gTIzs0qqxDWoVLZemYPf/mAyvr4jnZk2ROl9T25eX4j9eX260eBOg9L14la3p5shtpZk4O2Q2M8QpTMBV7Sn51JW065sLqm6e8Ob0pmAZdl9HZH+O2av5u5/pmbR+u2T8VcNej4ZX3Lw8GR8/J3FmJrSv8Zubmg2sFVigTKz+rh/8QomnnVTvz3/InfztR7wKT4zM6skL1BmZlZJXqDMKiDVqkPSvMS4syTtmYifKWnLG5GZVYivQZlVQ7dadUTEeV1jkhqAM4Frge5nHJlV3IBboF49Ip319XxbuvVOs9qT8bGNw4r7Hvxscuyf2tL1/3YetCIZf2jtzsn4irbic7Y2pP8/WdZaHAvl3X3LMgeHJfbfXnLgPLiktuCatnSG4L7NxfJvIxvS7+Hy9nRWXtnPbdmU9OtM9TyO9vTPeIBZC0yVdG1EvAAslzRc0uVkBWK/FRGXSvox8A1gHFlrjQ5gPnAQ8Jt8+2/3z0sw610DboEy20pdCuxM1qpjLfA+/taqo4OsZt+lXbYZBRwdEZFXTz8hIgrVeiVNJy+V1DgqtcSbVZOvQZlVQES0RcS5EXEQ8DlqWnXki06q7P28iE2Xco+IGRExJSKmNA6re5Nfs83mBcqsAiTtUZPksLFWHbVqz7G2UqyCbjageYEyq4butuoocz1wtaR/7u2JmfUXX4Myq4AetOo4s+bxWTWPfxf4bt/N0Kz+Kr1APfOZNxRiRwz/YXLsPesmJuMHDXkqGf/ZqrGF2LkPHZcc+7XXXJeMP9qyUzK+ui39cZRUN9ztmtYkx5Zl65Up68zbruJBcmNJ89Wy7L63jv1LMn7grR8pxC496uLk2IOGpDvnXrPssGT81EPnJuN3p85iVaSjrpn1rkovUGbWuw6YMJp5rodnA4SvQZmZWSV5gTIzs0ryAmVmZpVU12tQak4nD5Q1LJz27ju6ve9USR+AI4akm/NNnnliIdbRll6vJzUvTcbnrS3U7ARgVElDwFTzwPbk5y+hUelEhrLSTWXjl2wYWYiNbCo2awRo60i//kMGvZSMj5tZLIH01YnHJ8f+ZFI60aQsuWPnQcuT8ca9pxRi7QufSI41s4HNR1BmZlZJXqDM+liqlcZm7udDks7cyOOF9hxmA5nTzM36XrdaaZjZK/kIyqzvrQUOk7RjZJZLuiw/orpD0u4Aku6R9ANJcyV9Jo/tno/5NfDGPNYg6ff59jdLGrWxJ5c0XdI8SfOWLFnS16/VrNd4gTLre5cCC8haacyRtA8wPSKmAl8DPpiPGwOcBxwBnJzHPgWcExHHkxePjYgOYFq+/Q3Aezb25LXVzMePd7sNGzjqeoqvLFuvzEHDi+VxFrcWSxQBjG9amYwvbU9n1O276wuF2Ijm9NjHW7dLxhtKik03NaQz7cqy5FLKsvXKnrMn48tKHbWU/Dq80N6cjL98UHHfg9alGy02J0ouAUwelm5wWPbzfPi/ij//fd5X7Sy+iGgDzgXOzfs2nQO8JOkgYDDwQD50WUQ8CSBpXR7bm6whIcBd+WPDgR/mR15jgJ/X5YWY1ZmPoMz6WKKVxjhgx4g4Cvgyf+v1lPrrYyFwcH67M8f+OODZiHgjcDHpXlFmA56TJMz63gHAVZI6P4D2MeBCSTcDD25i268BV0j6JND54bA7gc9Jugl4DnimD+Zs1u+8QJn1sZJWGkclxqXaazwFHJnY7SEb295sa+BTfGZmVkleoMzMrJIqcYqv5fjXJeNvGFKsxXf1ygOTY98y4oFk/NpV+yfjv3jVrwqx7y3bNzn2wfUTkvGe1str6Si+3Y0lWXmDG3rWsLA1sW+AhsQcWyPR9A8Y17w6Gf/VyoOT8QmTi5mQn9+r65mszNyW4cn4kJLX2VHyt9MJ+99fiC1IjjSzga4SC5SZ1cf9i1cw8ayb+mz/i9wM0XqRT/GZmVkleYEyM7NK8gJlVhGSjsrr690m6VZJr+nmdmMkvbuv52dWb74GZVYBkrYHvg8cGxHP5/d36ebmY4B3A1f31fzM+kMlFqgn35Gu1LI20W11RGO6G+zLHUOT8aVt6eyxFR3FuoCLW8Ykx752+FPJ+BMtOyTjZVl8KzuGJOM92UdHpA96y3L+Ul18y7L4XjXkuWT8thWvTsb3Gl3stHv00LXJsdeu3ikZH96Qrn+4viNd/2/a2HsKsa/+n9OTY/XHPyfjFfU24LqIeB4gIl6W1CbpBmAU8AJwGjAWuBwYRFY26T3Ah4GjJc0CPhgRTmy0rYJP8ZlVw85A18q504GbIuJosoKypwDLgOPyOnxPAW8CfgDMjoipqcWptt1G+9oVffoizHqTFyizangW6PqBu72Au/Pbc8kqm28HXCtpNnAC3TgNWNtuo3GYeyXawOEFyqwabgL+QdJOAJK2IysC2/kp9tcDjwKnAr/Pj6puJKtk3gqkz9uaDWBeoMwqICKWAh8hq3o+G7iKrBnhCZJuA/YHfgbcCnxY0q+Azgt7zwFDJV0raVL9Z2/WNyqRJGFmEBG3A0d3CZ/Q5f6fydp3dHVcn0zKrB9VYoE69fA/JePLOwYVYjs1LU+MhCVto5LxYY3pLLEHN4wsxEY0pceuKskQLNNc0lF3cENbt/cxrCHdfXh1ezoTsKwuYGo/z7Wkr0OUZc6Vae0onlVa0Jp+7WUmNRczAQEe3rBjMj5ExXzFJ6alfz6T/tijqZhZxVRigTKz+jhgwmjmuV6eDRC+BmVmZpXkBcrMzCrJC5TZNqSv222Y9aZKXIM6ZcxdyfjyjsGF2PaN6aZ6L7ePSMaXtaZLHS1uHluItZeVESopDVSmrExRKkminXSZp1EN65LxFe09S9hIPeeatmLyCUBrpH8dhjamEzZWthV/PnPW7pUcO2nQi8n4kJL3quzn/HBL8XOpRxyVblZZbKdoZgOJj6DMzKySKnEEZbatkTSRrIzR/WTVIG4DvhwRZXV/zbY5PoIy6z+zI+JNZAVfG4B/73xAkv9t2jbPR1Bm/SwiQtKXgVslnQLMAUZL+gBwMVlB2NVk7TZ2AC4FWoBHImK6pB+TFZYN4IyIWFT/V2HW+7xAmVVARLRIGkzW7+mCiFgo6aPAzIi4RNK7yNpvLAMuj4gLJTVIagb2Aw7PF7rCkZek6fm2NI4aX7fXZLalKrFA7dmUzpL7w/phhdjEpmXJsYvbill5UJ6B90JrsdxPWbmgsn10lGTgUZIN2BNlmYCNFBsQZnPpvkElpZhWlTRULJtLymPr000c3zPy0WR8Tsv2yfj4xlXJ+NL2Ylbm+3ZI1zT6WrJkXTVJGgRsAJZFxMI8PBl4naQzgGbgduCHwOck/RS4JSJ+KunbwCWSVgCfJzva+l8RMQOYATB4533Sv0BmFVSJBcrM+CzwK7LW7Z0eBv4UEZcC5EdLTRFxVn7/QUmXA9dExBWSPgu8E/hpfadu1je8QJn1n6MlzSRLkLgDuIBXLlAzgBmS/im//01gRH7qbzDwW2AkcH1+aq8DOLlekzfra16gzPpBnsiQuiA0pWbMeuCMxJirutx/Y+/NzKw6nMpqZmaV5AXKbBtywITRLHK7DRsg6nqKr3H/fZPxZs1Pxp9vHVOITWpamhybamQHMK45XdMtpaEkQ25FWzGbEMqz20qz+xLKsvKGlDQsbCjJNGwvaTaYykAsa5y4rC1dt3BtonFkmQalX8+IhmLdPoDliaw8gO0b1iTjrR3FX9nJzSuSY5v22C0ZN7OBwUdQZmZWSU6SMNuGuN1G7/Lp0r7lIygzM6skL1BmZlZJXqDMepGkiZKWSJol6Y+S9i4ZNy//frakE+o7S7OBoa7XoNbsWax/B/DAhnRWWape3sjh6Sy2ic3p7L4FLTsn489tKO67LCuvvWQd70mNurLxZVl5Zdl9PZV+zvS+n11fzJoEGD8oXRdvhYrdfV9qKels3LE+GV/alh5/yOCnk/HV7cVswJEN6V/jlYcWu+/WyeyIOCkv8Ppp4AP1emJJDRHRk9KMZpXlIyizvvNX4DRJ3wCQ9Oq8NUaSpPMl3ZEffe0p6R8lfSp/bJSkm/Pbn5U0W9Jtkg7IY/dIuhD4SZ+/KrM68QJl1neOAhZ0Z6Ck1wE7R8SRwBeBLwA3Ap1pYicCv8wXpH0j4miyun3n5I93tuk4PbHv6ZLmSZrXvjb9mTGzKvICZdb7jpY0Czge+FhNfGOf4N6LrAU8wFxg74hYByzOr2O9C7iWrPfTG/L9/wwYlW9T26bjFSJiRkRMiYgpjcPSp9nNqsifgzLrfbMj4iQASQcCnSUtDt3INgvJjpIAXg90NtC6CvggWZuNFyQ9nO///fn+O0uI+LqTbXXqukC1Dk8fsC3vKF5sh/QF8daS3IHmkn+fLSUlgFoSJXMGN6WTNVo70n/4ljU4TLc3hGXtxZJJE0oSLZqVnkt5w8L0HFOvc01bunTR8KZ0eaWVbemfT8rQxnTJqUUlz7nP4OeT8bWR/tW8Z1mxfNH6cX9Kjn3uDd0vOdWH7geGSLqFbBFKioh5kp6TdAfQBnS22PgtcAnwqXzcfZIelTSbbFG6GTi3L1+AWX/xEZRZL8rbaJxUcz+AaYlxU/LvZ9fE/iMxrgXYvkvsq8BXU/sz25r4GpSZmVWSj6DMtiEHTBjNPNePswHCR1BmZlZJXqDMzKyS6nqKTx3pDLSycj+pkjwrSrLyXmxPl8wpa0LYlmjkt7Q13TyvbB89tby1mA2XaiiYxdM/mvLx3Y+vb0/ve3BjOnNwRUn5olTWX2uk/+Z5fMMOyfiYxrXJ+GOt45PxvUe+VIhdt3qf5NiGCeuScTMbGHwEZWZmleQFyszMKslZfGa9TNIg4Pf53UOB+fntEyJidf/Mymzg8QJl1ssiYgMwFbK+TxExtfbxvmyJIUn5HHrnwqlZP/IpPrM6kPRmSddLuh54r6RjJd0paa6k0/Mxl0l6dX77G5KOzL/uyltwfDF/7G15q405kt5ds+33gVuBkV2e+3+rmS9ZsqSur9tsS9T1CGrIy+kssfUlmXkpZdl6yzuKde6gvEbdds1rCrGy2npr29N15EY0tiTjZYYNK2a97dK8LDl2l6Z0W4TFjWOT8faS15nKQNx+cDpzriybsqk5HW9uKNYRbI/0PBasTzeO3G/o4mT82db06xzVVMzMG6J0DcG21rKqiP1mBHBMRISku4G3AGuAuZKuLtnmbcAXIuK3khokNQKfJTtC6wD+IOmafOzciPhI1x1ExAxgBsCUKVN8ZGUDho+gzOpnXs2pt4iIpXmtvYXATvCKvyY6V/rvAsdL+inw98COwD5kRWJvJavT11mr727MtiK+BmVWP7WHopK0HbAa2Bt4HlgG7CZpAXAI8AuyPk//JmkIWZ+oQ4CHgWMjolVSc/696/7NBjwvUGb943PAb/Lb34yIFkmXkLVsXwSszx/7iKRpwHDgRxHRLuk84BZJHWQL2yn1nbpZfXiBMutDNW01bgFuqYn/nr+lonfG/gy8tssu7gC+2WXcr4Ffd4md1nuzNqsGX4MyM7NKqusRVNPM+cn4XWv3SsYPHPZUIdZRsqbu1ZxOn52zKl2nbeqohwuxsmzCOav2TsYXrxuTjP/99g8k4w+t26UQu+y5I5Jjh5R0pl3Wks5WHD0oXXdu12HLC7GOkky7F9aPTsbft/OcZDyVaffMhnT23bjmVcn41KHPJuP3N6Y/z3pPTCzEHmvZMTl2v7OXJuP4WMNsQPARlJmZVZKvQZltQ+5fvIKJZ930itgiNzC0ivIRlJmZVZIXKDMzqyQvUGbdIGlQXg9vlqRVNbfTtbeybeYlYmdKKmTGSDpR0g419/eT9PWucbNtSSWuQT25bvtk/A8vvqoQG96crrt2+NgnkvFHTp+UjD/0YLouYFo6oy774H/RlRSz9co9n4yWVflr4OVkPJ0jBw8lo2UdH9LxGaTfwyQlEp0hAAAGy0lEQVSlMwSv/8Lbk/G9Tn8xGb9x2UHJ+PaDinO8d/luybFtjy9KxjfHpiqU92A/P+4ak9QAnEhW8qjzDTmO7IO8Z3SJm20zfARl1gskHZFXHZ8t6Zw83CDpB3nF8s/k486WdIKkiZJuzwu9/ifZgvQjSV/Jtz0SWFwblzRa0g35c1ydH9VNlXRT/nWXpPTnKswGoEocQZltBY4HzomIG/MjIoAxwHnA08C9wFe6bLMLWXXzDZL2A74REX+VNBRojYgFkn5bE/9P4KaIuChvvXEK8CQwGjgKOIKshNKZtU8iaTowHaBx1Phef+FmfcVHUGabSdLH8+tQHwe+BxybVx0/Lh+yLCKezJsTpj5J/Zf81GFXU4HZifhe/K1i+VyyIrMA9+ZV0ufnY14hImZExJSImNI4LP1hbLMq8hGU2WaKiG8B3wKQNDQiPpa3e59PVitvU72XaquPtwKdDazeAnw7EV8IvC7f/+uBR/P4QXkn3YOBxzb7BZlVTF0XqEcuOiwZf/rpdAO93U76ayFWljwwm6Flz9qNmVmvKek0vvt/p8slnf/f+5XsKP2TfpRiOaoD73khOfaBycUkmz70QUnvJKs6/uPN2P43wAWSfgfsGRFPJOI/AC6X9F6y7JqvAG8gy5G5CRgHnLpFr8KsQnwEZdZDnRXKu8QuAC4oGxcRh+ffz64ZclLN49cB10kaTNbv6RXxmm1OqH2OvA/UgxHxyc14KWaV5gXKrELyDru/7O95mFWBFyizASwiZgGzujv+gAmjmefaezZAOIvPzMwqyQuUmZlVUl1P8e1xQzrDa9WEkVu8bzWlX0q09aCkUUPjpsd0R3Rseszm0pb/TaGGdDmiMtFRki3dk9dZNu+O9h7NJeVXjxyQjO/54H1bvG8z6z8+gjIzs0ryAmVmZpXkBcrMzCrJaeZm25D58+evlrSgv+fRxTjgpf6eRBee06ZtyXz26M4gL1Bm25YFqUoY/Snvr+U5bULV5lSP+dR1gbrthk/1LH3soj6aiG0b3t3fEzCzLeFrUGZmVkleoMy2LTP6ewIJnlP3VG1OfT4fRUl7BDMzs/7kIygzM6skL1BmZlZJXqDMthKSjpO0QNJCSWclHh8s6ar88bmSJtY89pk8vkDSW+o0n49LelDSfZJulbRHzWPtkv6cf13fG/Pp5pzOlLSk5rnfX/PY+yQ9mn+9r45zOr9mPo9IWl7zWK+/T5IukfSipGJL8+xxSfpOPt/7JB1S81jvvkcR4S9/+WuAfwGNwGPAJGAQ8BdgcpcxHwEuym+fDFyV356cjx8M7Jnvp7EO8/k7YFh++8Od88nvr+6n9+hM4MLEttsBj+ffx+a3x9ZjTl3G/ytwSR+/T28EDgH+WvL48cBvAAGHA3P76j3yEZTZ1uEwYGFEPB4RG4CfAdO6jJkG/CS/fS1wjLKe8dOAn0VES0Q8ASzM99en84mIP0TE2vzuncCuW/icWzynjXgLcHNELI2IZcDNwHH9MKdTgCt74XlLRcRtwNKNDJkG/DQydwJjJO1MH7xHXqDMtg4TgKdr7j+Tx5JjIqINWAFs381t+2I+tf6Z7K/yTkMkzZN0p6QTt3AuPZ3Tu/JTV9dK2q2H2/bVnMhPge4JzKwJ98X7tCllc+7198iljsy2DqkqLV0/Q1I2pjvb9sV8soHSacAU4Oia8O4R8aykScBMSfdHxGN1mNMNwJUR0SLpQ2RHnG/q5rZ9NadOJwPXRkRtE7W+eJ82pW6/Rz6CMts6PAPsVnN/V+DZsjGSmoDRZKdyurNtX8wHSW8GPge8IyJaOuMR8Wz+/XFgFnDwFs6nW3OKiJdr5vH/gEO7u21fzanGyXQ5vddH79OmlM2599+j3r7A5i9/+av+X2RnQx4nOwXUebF9/y5j/oVXJklcnd/en1cmSTzOlidJdGc+B5MlCOzTJT4WGJzfHgc8ykYSB3p5TjvX3P4H4M789nbAE/ncxua3t6vHnPJx+wKLyIsr9OX7lO9vIuVJEm/jlUkSd/XVe+RTfGZbgYhok/RR4HdkmWGXRMQDks4B5kXE9cD/AJdKWkh25HRyvu0Dkq4GHgTagH+JV55G6qv5fB0YAVyT5WrwVES8A9gP+KGkDrKzPOdFxINbMp8ezOnfJL2D7H1YSpbVR0QslfQl4O58d+dExMYSCXpzTpAlR/ws8pUg1yfvk6QrganAOEnPAF8EmvP5XgT8miyTbyGwFvin/LFef49c6sjMzCrJ16DMzKySvECZmVkleYEyM7NK8gJlZmaV5AXKzMwqyQuUmZlVkhcoMzOrJC9QZmZWSV6gzMyskv4/wo8Cgej1MCcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import helper module (should be in the repo)\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Calculate the class probabilities (softmax) for img\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "\n",
    "ps = torch.exp(output)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next part, I'll show you how to save your trained models. In general, you won't want to train a model everytime you need it. Instead, you'll train once, save it, then load the model when you want to train more or use if for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
